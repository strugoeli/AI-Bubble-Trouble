{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Project__Bubble_Trouble (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu6d7kjzSF_1",
        "colab_type": "text"
      },
      "source": [
        "#**Welcome to the Bubble Trouble AI Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8JTy9fPP6u0",
        "colab_type": "text"
      },
      "source": [
        "#**Note :** \n",
        "#Before connecting make sure that\n",
        "#the directory 'gym-bubble-trouble' is at the directory 'My Drive'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eD3QiqNeytk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "662f488d-9ad6-4c6f-8434-0c4f95067957"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 267kB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ5Yxk_hUnYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "b27c940b-e518-4680-c68b-1a8394b888e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEPFTjKsUSCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ab30327d-48ba-4ad9-840d-0959891d9901"
      },
      "source": [
        "\n",
        "%cd drive/My\\ Drive/gym-bubble-trouble"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/gym-bubble-trouble\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1C8OraTnDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('bubbletrouble')\n",
        "!mkdir -p ../models"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1gYiQsbRvRD",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bh2FI10V-TT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f63575a7-2246-4ea7-ab54-52c1a0371572"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import gym_bubbletrouble\n",
        "import cv2 as cv\n",
        "from torchvision import utils\n",
        "\n",
        "\n",
        "from settings import *\n",
        "import time\n",
        "from game import BubbleTroubleGame\n",
        "import pygame\n",
        "from pygame.locals import K_LEFT, K_RIGHT, K_SPACE, K_ESCAPE, KEYUP, KEYDOWN, QUIT\n",
        "\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfn1sydbksv",
        "colab_type": "text"
      },
      "source": [
        "# DQN Architecture\n",
        "Deep-Q-Networks (DQNs) are composed of: \n",
        "* 3 convolution layers\n",
        "* 2 fully-connected linear layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81sfKF-vAYkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions=4):\n",
        "        \"\"\"\n",
        "        Initialize Deep Q Network\n",
        "        :param in_channels: Number of input channels\n",
        "        :param n_actions: Number of outputs\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, NUM_FILTER_L1, kernel_size=KER_L1, stride=STRIDE_L1)\n",
        "        self.bn1 = nn.BatchNorm2d(NUM_FILTER_L1)\n",
        "        self.conv2 = nn.Conv2d(NUM_FILTER_L1, NUM_FILTER_L2, kernel_size=KER_L2, stride=STRIDE_L2)\n",
        "        self.bn2 = nn.BatchNorm2d(NUM_FILTER_L2)\n",
        "        self.conv3 = nn.Conv2d(NUM_FILTER_L2, NUM_FILTER_L3, kernel_size=KER_L3, stride=STRIDE_L3)\n",
        "        self.bn3 = nn.BatchNorm2d(NUM_FILTER_L3)\n",
        "\n",
        "        def conv2d_size_out(size, kernel_size, stride):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        width_l1 = conv2d_size_out(WIDTH, KER_L1, STRIDE_L1)\n",
        "        width_l2 = conv2d_size_out(width_l1, KER_L2, STRIDE_L2)\n",
        "        convw = conv2d_size_out(width_l2, KER_L3, STRIDE_L3)\n",
        "\n",
        "        heigt_l1 = conv2d_size_out(HEIGHT, KER_L1, STRIDE_L1)\n",
        "\n",
        "        height_l2 = conv2d_size_out(heigt_l1, KER_L2, STRIDE_L2)\n",
        "        convh = conv2d_size_out(height_l2, KER_L3, STRIDE_L3)\n",
        "\n",
        "        linear_input_size = convw * convh * NUM_FILTER_L3\n",
        "\n",
        "        self.fc4 = nn.Linear(linear_input_size, linear_input_size)\n",
        "        self.head = nn.Linear(linear_input_size, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.fc4(x.reshape(x.size(0), -1)))\n",
        "        return self.head(x)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW-ekr-Ibxtl",
        "colab_type": "text"
      },
      "source": [
        "# Normal Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtklqh1EYu43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.push_count = 0\n",
        "\n",
        "    def push(self, experience):\n",
        "        \"\"\"\n",
        "        Push the given experience to the memory buffer\n",
        "        :param experience: Experience object contains ('state', 'action', 'next_state', 'reward', 'done')\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count % self.capacity] = experience\n",
        "        self.push_count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        :param batch_size: Number of samples\n",
        "        :return: list of samples in size of the given batch size\n",
        "        \"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        :return: True if there enough samples to sample and false otherwise\n",
        "        \"\"\"\n",
        "        return len(self.memory) >= batch_size\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olv_b06Hbd47",
        "colab_type": "text"
      },
      "source": [
        "# Prioritized Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Jqf4C54Igw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, maxlen):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "        self.priorities = deque(maxlen=maxlen)\n",
        "\n",
        "    def push(self, experience):\n",
        "        \"\"\"\n",
        "        Push the given experience to the memory buffer\n",
        "        :param experience: Experience object contains ('state', 'action', 'next_state', 'reward', 'done')\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.priorities.append(max(self.priorities, default=1))\n",
        "\n",
        "    def get_probabilities(self, priority_scale):\n",
        "        \"\"\"\n",
        "        :param priority_scale: scale factor is a number in [0,1]\n",
        "        :return: The current sample probabilities\n",
        "        \"\"\"\n",
        "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
        "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
        "        return sample_probabilities\n",
        "\n",
        "    def get_importance(self, probabilities):\n",
        "        \"\"\"\n",
        "        :param probabilities: sample probabilities\n",
        "        :return: The current importance\n",
        "        \"\"\"\n",
        "        importance = 1 / len(self.buffer) * 1 / probabilities\n",
        "        importance_normalized = importance / max(importance)\n",
        "        return importance_normalized\n",
        "\n",
        "    def sample(self, batch_size, priority_scale=1.0):\n",
        "        \"\"\"\n",
        "        :param batch_size: Number of samples\n",
        "        :param priority_scale: scale factor is a number in [0,1]\n",
        "        :return: list of samples in size of the given batch_size, there importance\n",
        "        and there indices in the memory buffer\n",
        "        \"\"\"\n",
        "        sample_size = min(len(self.buffer), batch_size)\n",
        "        sample_probs = self.get_probabilities(priority_scale)\n",
        "        sample_indices = random.choices(range(len(self.buffer)), k=sample_size, weights=sample_probs)\n",
        "        samples = [self.buffer[i] for i in sample_indices]\n",
        "        importance = self.get_importance(sample_probs[sample_indices])\n",
        "        return samples, importance, sample_indices\n",
        "\n",
        "    def set_priorities(self, indices, errors, offset=0.1):\n",
        "        \"\"\"\n",
        "         Sets new priorities to the experiences which there indices are given\n",
        "        \"\"\"\n",
        "        for i, e in zip(indices, errors):\n",
        "            self.priorities[i] = abs(e) + offset\n",
        "\n",
        "    def can_provide_sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        :return: True if there enough samples to sample and false otherwise\n",
        "        \"\"\"\n",
        "        return len(self.buffer) >= batch_size"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHZnFulNbNh-",
        "colab_type": "text"
      },
      "source": [
        "# Exploration - Exploitation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPyQ5iRHYxJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedyStrategy:\n",
        "    \"\"\"\n",
        "    This class responsible for providing the exploration - exploitation strategy\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "\n",
        "    def get_exploration_rate(self, curr_step):\n",
        "        \"\"\"\n",
        "        :param curr_step: the current step number\n",
        "        :return: exploration rate\n",
        "        \"\"\"\n",
        "        return self.end + (self.start - self.end) * math.exp(-curr_step / self.decay)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWVPllBtbIfy",
        "colab_type": "text"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_28FX5JPYziZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    This class ris responsible for training and testing the model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, policy_net, target_net, strategy, em, test_em, num_actions, optimizer):\n",
        "        self.curr_step = 0\n",
        "        self.rate = 0\n",
        "        self.strategy = strategy\n",
        "        self.num_actions = num_actions\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.policy_net, self.target_net = policy_net, target_net\n",
        "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
        "        self.optimizer = optimizer\n",
        "        self.em = em\n",
        "        self.test_em = test_em\n",
        "        self.scale = 0.7\n",
        "\n",
        "    def select_action(self, state, actions):\n",
        "        \"\"\"\n",
        "        This method is given state and actions that can be taken from this state\n",
        "        then sample randomly number k in (0,1].\n",
        "        if rate > k samples randomly number from the given actions  and returns it\n",
        "        else returns the action with the highest q-value from the given actions\n",
        "\n",
        "        :param state: Tensor\n",
        "        :param actions: list of int - action space for the given state\n",
        "        :return: int - selected action\n",
        "        \"\"\"\n",
        "        self.rate = self.strategy.get_exploration_rate(self.curr_step)\n",
        "        self.curr_step += 1\n",
        "        if self.rate > random.random():\n",
        "            return random.choice(actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_step(state, actions)\n",
        "\n",
        "    def policy_step(self, state, actions):\n",
        "        \"\"\"\n",
        "        This method is given state and actions that can be taken from this state and returns\n",
        "        the action with the highest q-value from the given actions\n",
        "        :param state: Tensor\n",
        "        :param actions: list of int - action space for the given state\n",
        "        :return: int - selected action\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            output = self.policy_net(state.to(self.device))\n",
        "            top = output.topk(4).indices[0].cpu().numpy()\n",
        "            for act in top:\n",
        "                if act in actions:\n",
        "                    return act\n",
        "\n",
        "    def test(self, n_episodes):\n",
        "        \"\"\"\n",
        "        This method is given the number of episides to test the model and tests it\n",
        "        :param n_episodes: number of episodes to train the model\n",
        "        :return: The average reward of the test\n",
        "        \"\"\"\n",
        "        avg_reward = 0\n",
        "        for episode in range(n_episodes):\n",
        "            state = self.test_em.reset()\n",
        "            total_reward = 0.0\n",
        "            for _ in count():\n",
        "\n",
        "                action = self.policy_step(state, self.test_em.get_legal_actions())\n",
        "                next_state, reward, done, info = self.test_em.step(action)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    print(\"Finished Episode {} with reward {}\".format(episode, total_reward))\n",
        "                    avg_reward += total_reward\n",
        "                    break\n",
        "\n",
        "        avg_reward = avg_reward / n_episodes if n_episodes else 0\n",
        "        print(\"Avarage reward over {} espisodes is: {}\".format(n_episodes, avg_reward))\n",
        "        return avg_reward / n_episodes\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        \"\"\"\n",
        "        This method is given the number of episides to train the model and trains it\n",
        "        :param num_episodes: number of episodes to train the model\n",
        "        \"\"\"\n",
        "        num_steps = cum_reward = max_reward_train = 0\n",
        "        episode_reward = []\n",
        "        episode_loss = []\n",
        "        for episode in range(num_episodes):\n",
        "\n",
        "            state = self.em.reset()\n",
        "            cum_loss = 0\n",
        "\n",
        "            for time_step in count():\n",
        "\n",
        "                num_steps += 1\n",
        "                action = self.select_action(state, self.em.get_legal_actions())\n",
        "                next_state, reward, done, _ = self.em.step(action)\n",
        "                reward = torch.tensor([reward], device=self.device)\n",
        "                self.memory.push(Experience(state, action, next_state, reward, done))\n",
        "                state = next_state\n",
        "                cum_reward += reward.item()\n",
        "\n",
        "                if self.memory.can_provide_sample(BATCH_SIZE):\n",
        "                    cum_loss = self._train_step(cum_loss)\n",
        "\n",
        "                if num_steps % TARGET_UPDATE == 0:\n",
        "                    # Update the target net\n",
        "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "          \n",
        "                if self.em.done:\n",
        "                    max_reward_train = max(max_reward_train, cum_reward)\n",
        "                    episode_reward.append(cum_reward)\n",
        "                    mean_loss = cum_loss / (time_step + 1)\n",
        "                    episode_loss.append(mean_loss)\n",
        "                    plot(episode_reward, 20, 'Reward')\n",
        "                    plot(episode_loss, 20, 'Loss')\n",
        "                    if is_ipython: display.clear_output(wait=True)\n",
        "                    cum_reward =0\n",
        "                    break\n",
        "\n",
        "\n",
        "    def _train_step(self, cum_loss):\n",
        "\n",
        "        # Sampling from memory a batch of experiences\n",
        "        experiences, importance, indices = self.memory.sample(BATCH_SIZE, priority_scale=self.scale)\n",
        "        states, actions, rewards, next_states, dones = extract_tensors(experiences, self.device)\n",
        "\n",
        "        # Get the Q-values and the target values from the experiences\n",
        "        current_q_values = QValues.get_current(self.policy_net, states, actions)\n",
        "        next_q_values = QValues.get_next(self.target_net, next_states)\n",
        "        target_q_values = (next_q_values * GAMMA) * (1 - dones).squeeze(1) + rewards\n",
        "        weights = importance ** (1 - self.rate)\n",
        "        errors, loss = get_loss(current_q_values, target_q_values, weights, BATCH_SIZE, self.device)\n",
        "\n",
        "        # Update the priorities\n",
        "        self.memory.set_priorities(indices, errors)\n",
        "\n",
        "        # Finish the training step\n",
        "        cum_loss += loss.item()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "        return cum_loss\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0wv__Xa7Ow",
        "colab_type": "text"
      },
      "source": [
        "# Gym Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv3Qyw0VZfmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype='uint8')\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        info = None\n",
        "        danger = self.env.closest_dist_euc < 60 or not self.env.can_shoot()\n",
        "\n",
        "        num_skip = 1 if danger else self._skip\n",
        "        for i in range(num_skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "            if i == num_skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == num_skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJXneLip27aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "          \n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(3)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.lives()\n",
        "        return obs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fawkr-4QZOxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not believe how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "        self._out = None\n",
        "\n",
        "    def _force(self):\n",
        "        if self._out is None:\n",
        "            self._out = np.concatenate(self._frames, axis=-1)\n",
        "            self._frames = None\n",
        "        return self._out\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = self._force()\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._force())\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._force()[i]\n",
        "\n",
        "    def count(self):\n",
        "        frames = self._force()\n",
        "        return frames.shape[frames.ndim - 1]\n",
        "\n",
        "    def frame(self, i):\n",
        "        return self._force()[..., i]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busfAJM2ZRve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = (HEIGHT, WIDTH, 1)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k))\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        obs = self._get_ob()\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy2DwD70bByb",
        "colab_type": "text"
      },
      "source": [
        "# Q-Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3uLCAN0ZkUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QValues:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_current(policy_net, states, actions):\n",
        "        \"\"\"\n",
        "        given batch of states and actions and returns there current q values calculated by the given policy network\n",
        "        \"\"\"\n",
        "        return policy_net(states.to(QValues.device)).gather(1, actions)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_next(target_net, next_states):\n",
        "        \"\"\"\n",
        "        Given batch of states and actions and returns there target values calculated by the given target network\n",
        "        \"\"\"\n",
        "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0]\n",
        "        final_state_locations = final_state_locations.eq(0).type(torch.bool)\n",
        "        non_final_state_locations = (final_state_locations == False)\n",
        "        non_final_states = next_states[non_final_state_locations]\n",
        "        batch_size = next_states.shape[0]\n",
        "        values = torch.zeros(batch_size).to(QValues.device)\n",
        "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
        "        return values\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Ciu7xrae5P",
        "colab_type": "text"
      },
      "source": [
        "# Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwzGcxSIZ_vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ACTION_LEFT = 0\n",
        "ACTION_RIGHT = 1\n",
        "ACTION_FIRE = 2\n",
        "ACTION_IDLE = 3\n",
        "\n",
        "key_map = {0: K_LEFT, 1: K_RIGHT, 2: K_SPACE, 3: None}\n",
        "DEFAULT_REWARDS = {'moving': 0, 'fire': 0, 'score': 1, 'death': -1, 'win': 0, 'step': 0}\n",
        "\n",
        "\n",
        "\n",
        "class BubbleTroubleEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['rgb_array']}\n",
        "\n",
        "    def __init__(self, rewards=None):\n",
        "        pygame.init()\n",
        "        self.rewards = rewards if rewards else DEFAULT_REWARDS\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        self.n_steps = 0\n",
        "        self.previous_score = None\n",
        "        self.game = None\n",
        "        self.closest_dist_euc, self.closest_ball_euc = 0, None\n",
        "        self.closest_dist, self.closest_ball = 0, None\n",
        "        # Init game\n",
        "        self.surface = pygame.Surface((WINDOWWIDTH, WINDOWHEIGHT), pygame.RESIZABLE)\n",
        "\n",
        "    def lives(self):\n",
        "        \"\"\"\n",
        "        :return: Current number of lives of the player\n",
        "        \"\"\"\n",
        "        return self.game.player.lives\n",
        "\n",
        "    def can_shoot(self):\n",
        "        \"\"\"\n",
        "        :return: True if the player can shoot and false otherwise\n",
        "        \"\"\"\n",
        "        return self.game.player.can_shoot()\n",
        "\n",
        "    def get_ball_list(self):\n",
        "        \"\"\"\n",
        "        :return: List of all the ball objects in the game\n",
        "        \"\"\"\n",
        "        return self.game.balls + self.game.hexagons\n",
        "\n",
        "    def _update_closest_ball(self):\n",
        "        # Update the closest ball and it's distance from the player\n",
        "        # One for euclidean distance and once for x axis distance\n",
        "        balls_list = self.get_ball_list()\n",
        "        if balls_list:\n",
        "            self.closest_ball, self.closest_dist = get_x_axis_closest_bubble(self.game, balls_list)\n",
        "            self.closest_ball_euc, self.closest_dist_euc = get_euclidian_closest_bubble(self.game, balls_list)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment and returns the first observation\n",
        "        \"\"\"\n",
        "        self.n_steps = 0\n",
        "        self.game = BubbleTroubleGame()\n",
        "        self.game.load_level(1)\n",
        "        self.previous_score = self.game.score\n",
        "        return self._get_processed_screen()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute a single step of the game\n",
        "        :param action: int - action to execute\n",
        "        \"\"\"\n",
        "        self.n_steps += 1\n",
        "        self._make_single_step(action)\n",
        "        state = self._get_processed_screen()\n",
        "        win = self.game.is_completed\n",
        "        done = self.is_done()\n",
        "        reward = self._fitness(action, not self.game.player.is_alive, win, self._is_ball_hit())\n",
        "        self.previous_score = self.game.score\n",
        "        return state, reward, done, {}\n",
        "\n",
        "    def is_done(self):\n",
        "        \"\"\"\n",
        "        :return: True if the episode is over and false otherwise\n",
        "        \"\"\"\n",
        "        return self.game.game_over or self.game.is_completed\n",
        "\n",
        "    def render(self, mode='rgb_array', *args, **kwargs):\n",
        "        \"\"\"\n",
        "        :return: current frame of the game\n",
        "        \"\"\"\n",
        "        image = pygame.surfarray.array3d(self.surface)\n",
        "        return image.swapaxes(1, 2).transpose((2, 0, 1))\n",
        "\n",
        "    def _get_processed_screen(self):\n",
        "        screen = self.render()\n",
        "        screen = cv.cvtColor(screen, cv.COLOR_RGB2GRAY)\n",
        "        screen = cv.resize(screen, (WIDTH, HEIGHT), interpolation=cv.INTER_AREA)\n",
        "        screen = np.expand_dims(screen, -1)\n",
        "        return screen\n",
        "\n",
        "    def close(self):\n",
        "        self.game.exit_game()\n",
        "\n",
        "    def _fitness(self, action, dead, win, score_change):\n",
        "        fitness = self.rewards['step']\n",
        "        if action == ACTION_FIRE:\n",
        "            fitness += self.rewards['fire']\n",
        "        elif action != ACTION_IDLE:\n",
        "            fitness += self.rewards['moving']\n",
        "        if dead:\n",
        "            fitness += self.rewards['death']\n",
        "        if win:\n",
        "            fitness += self.rewards['win']\n",
        "        if score_change:\n",
        "            fitness += self.rewards['score']\n",
        "        return float(fitness)\n",
        "\n",
        "    def _is_ball_hit(self):\n",
        "        # True if an object is been destroyed and false otherwise\n",
        "        return self.previous_score != self.game.score\n",
        "\n",
        "    def _make_single_step(self, action):\n",
        "        # Execute a single step of the game by drawing and updating the game image\n",
        "        key = key_map[action]\n",
        "        self.handle_key(key)\n",
        "        self.game.update()\n",
        "        self.draw_world()\n",
        "\n",
        "    def render_with_states(self):\n",
        "\n",
        "        img = np.ascontiguousarray(self.render(), dtype=np.uint8)\n",
        "        c_x = int(self.game.player.position())\n",
        "        closest_ball = self.closest_ball_euc.rect\n",
        "        x, y = int(closest_ball.centerx), int(closest_ball.centery)\n",
        "        p1, p2 = closest_ball.topleft, closest_ball.bottomright\n",
        "        img = cv.rectangle(img, p1, p2, GREEN, 2)\n",
        "        img = cv.line(img, (x, y), (c_x, WINDOWHEIGHT), GREEN, 1)\n",
        "        return img\n",
        "\n",
        "    def handle_key(self, key):\n",
        "        game = self.game\n",
        "        if key not in key_map.values():\n",
        "            print('Wrong key')\n",
        "            return\n",
        "        if key == K_LEFT:\n",
        "            game.move_player(direction=1)\n",
        "        elif key == K_RIGHT:\n",
        "            game.move_player(direction=-1)\n",
        "        elif key == K_SPACE:\n",
        "            game.fire_player()\n",
        "        elif key is None:\n",
        "            game.stop_player()\n",
        "\n",
        "    def draw_world(self):\n",
        "        self.surface.fill(WHITE)\n",
        "        for hexagon in self.game.hexagons:\n",
        "            self.draw_hex(hexagon)\n",
        "        for ball in self.game.balls:\n",
        "            self.draw_ball(ball)\n",
        "        self._update_closest_ball()\n",
        "        if self.game.player.weapon.is_active:\n",
        "            self.draw_weapon(self.game.player.weapon)\n",
        "        self.draw_player(self.game.player)\n",
        "\n",
        "    def draw_ball(self, ball):\n",
        "        self.surface.blit(ball.image, ball.rect)\n",
        "\n",
        "    def draw_hex(self, hexagon):\n",
        "        self.surface.blit(hexagon.image, hexagon.rect)\n",
        "\n",
        "    def draw_player(self, player):\n",
        "        self.surface.blit(player.image, player.rect)\n",
        "\n",
        "    def draw_weapon(self, weapon):\n",
        "        self.surface.blit(weapon.image, weapon.rect)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWB18NcAas6V",
        "colab_type": "text"
      },
      "source": [
        "# Environment Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jQPU2D3Y7qJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "TOO_LOW = 80\n",
        "TOO_CLOSE = 50\n",
        "TOO_FAR = 320\n",
        "TOO_FAR_X = 150\n",
        "\n",
        "ACTION_LEFT = 0\n",
        "ACTION_RIGHT = 1\n",
        "ACTION_FIRE = 2\n",
        "ACTION_IDLE = 3\n",
        "\n",
        "reward_dict = {'moving': .0, 'fire': .0, 'score': 1, 'death': -1., 'win': 0, 'step': .0}\n",
        "\n",
        "\n",
        "class EnvManager:\n",
        "    \"\"\"\n",
        "    This class is responsible for managing the Bubble trouble environment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, num_frames=4, skip=2, to_skip=True, ep_live=False):\n",
        "        self.device = device\n",
        "        self.env = BubbleTroubleEnv(rewards=reward_dict)\n",
        "        self.env = FrameStack(self.env, num_frames)\n",
        "        if to_skip:\n",
        "            self.env = MaxAndSkipEnv(self.env, skip)\n",
        "        if ep_live:\n",
        "            self.env = EpisodicLifeEnv(self.env)\n",
        "        self.done = False\n",
        "        self.env.reset()\n",
        "        self.curr_screen = None\n",
        "        self.frame_counter = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment and returns the first observation\n",
        "        \"\"\"\n",
        "        ob = self.env.reset()\n",
        "        ob = get_state(ob)\n",
        "        return ob\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute a single step of the game\n",
        "\n",
        "        :param action: int - action to execute\n",
        "        :return: observation - Tensor, reward -int, done -boolean, info -Not in use\n",
        "        \"\"\"\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        ob = get_state(ob)\n",
        "        self.done = done\n",
        "        return ob, reward, done, info\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        :return: current frame of the game\n",
        "        \"\"\"\n",
        "        return self.env.render('rgb_array')\n",
        "\n",
        "    def num_actions_available(self):\n",
        "        \"\"\"\n",
        "        :return: Number of steps\n",
        "        \"\"\"\n",
        "        return self.env.action_space.n\n",
        "\n",
        "    def just_starting(self):\n",
        "        \"\"\"\n",
        "        :return: True if is the first frame and false otherwise\n",
        "        \"\"\"\n",
        "        return self.curr_screen is None\n",
        "\n",
        "    def can_shoot(self):\n",
        "        \"\"\"\n",
        "        :return: True if the player may shoot and false otherwise\n",
        "        \"\"\"\n",
        "        return self._get_player().can_shoot()\n",
        "\n",
        "    def get_legal_actions(self):\n",
        "        \"\"\"\n",
        "        :return:  Returns set of legal action minus 'irrational' if exist\n",
        "        \"\"\"\n",
        "        player = self._get_player()\n",
        "        actions = []\n",
        "        logic_actions = self.get_logic_actions()\n",
        "        if logic_actions:\n",
        "            return logic_actions\n",
        "        if player.rect.left > 0:\n",
        "            actions.append(ACTION_LEFT)\n",
        "        if player.rect.right < WINDOWWIDTH:\n",
        "            actions.append(ACTION_RIGHT)\n",
        "        if player.can_shoot() and self.env.closest_dist < TOO_FAR_X :\n",
        "            actions.append(ACTION_FIRE)\n",
        "        actions.append(ACTION_IDLE)\n",
        "        return actions\n",
        "\n",
        "    def get_logic_actions(self):\n",
        "        \"\"\"\n",
        "        :return: Returns 'logical' set of action that the player may execute\n",
        "        \"\"\"\n",
        "        if not self.env.closest_ball_euc:\n",
        "            return []\n",
        "\n",
        "        player = self._get_player()\n",
        "        ball_rec = self.env.closest_ball_euc.rect\n",
        "        on_the_left = ball_rec.centerx <= player.rect.centerx\n",
        "\n",
        "        if self._is_in_danger(ball_rec):\n",
        "\n",
        "            if on_the_left:\n",
        "                return [ACTION_RIGHT, ACTION_FIRE] if player.can_shoot() else [ACTION_RIGHT]\n",
        "            else:\n",
        "                return [ACTION_LEFT, ACTION_FIRE] if player.can_shoot() else [ACTION_LEFT]\n",
        "\n",
        "        if self._is_too_far() and self._is_normal_ball():\n",
        "\n",
        "            if on_the_left:\n",
        "                return [ACTION_LEFT]\n",
        "            else:\n",
        "                return [ACTION_RIGHT]\n",
        "\n",
        "    def _is_in_danger(self, ball_rec):\n",
        "        # True if the player in danger zone and false otherwise\n",
        "        too_low = WINDOWHEIGHT - ball_rec.centery <= TOO_LOW\n",
        "        player = self._get_player()\n",
        "        return self._is_too_close() and (not player.can_shoot() or player.can_shoot() and too_low)\n",
        "\n",
        "    def _is_normal_ball(self):\n",
        "        # Normal means not Hexagon\n",
        "        return self.env.closest_ball_euc in self.env.game.balls\n",
        "\n",
        "    def _is_too_close(self):\n",
        "        # True if too close to the closest ball and false otherwise\n",
        "        return self.env.closest_dist_euc < TOO_CLOSE\n",
        "\n",
        "    def _is_too_far(self):\n",
        "        # True if too far from the closest ball and false otherwise\n",
        "        return self.env.closest_dist > TOO_FAR\n",
        "\n",
        "    def _get_game(self):\n",
        "        return self.env.game\n",
        "\n",
        "    def _get_player(self):\n",
        "        return self._get_game().player\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oe-p4L-cG1_",
        "colab_type": "text"
      },
      "source": [
        "#Utill"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORRTyVRNT5SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "\n",
        "def init_networks(num_channels, num_actions, model, device):\n",
        "    \"\"\"\n",
        "    :param num_channels: Number of channels in the input\n",
        "    :param num_actions: Number of outputs\n",
        "    :param model: The network\n",
        "    :param device: Current device\n",
        "    :return: initialized policy and target networks\n",
        "    \"\"\"\n",
        "    policy_net = model(num_channels, num_actions).to(device)\n",
        "    target_net = model(num_channels, num_actions).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "    return policy_net, target_net\n",
        "\n",
        "\n",
        "def load_model(policy, target, optimizer, path):\n",
        "    \"\"\"\n",
        "    Load trained model from the given path\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(path, map_location={'cuda:0': 'cpu'})\n",
        "    policy.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    target.load_state_dict(policy.state_dict())\n",
        "    target.eval()\n",
        "\n",
        "\n",
        "def get_state(obs):\n",
        "    \"\"\"\n",
        "    Converting the given observation to Tensor and returns it\n",
        "    \"\"\"\n",
        "    state = np.array(obs)\n",
        "    state = state.transpose((2, 0, 1))\n",
        "    state = torch.from_numpy(state)\n",
        "    return state.unsqueeze(0)\n",
        "\n",
        "\n",
        "def extract_tensors(experiences, device):\n",
        "    \"\"\"\n",
        "    Extracts the given experiences into tuple of Tensors and returns it\n",
        "    \"\"\"\n",
        "    # Convert batch of Experiences to Experience of batches\n",
        "    batch = Experience(*zip(*experiences))\n",
        "    actions_b = tuple((map(lambda a: torch.tensor([[a]], device=device), batch.action)))\n",
        "    rewards_b = tuple((map(lambda r: torch.tensor([r], device=device), batch.reward)))\n",
        "    dones_b = tuple((map(lambda d: torch.tensor([[float(d)]], device=device), batch.done)))\n",
        "\n",
        "    t1 = torch.cat(batch.state).to(device)\n",
        "    t2 = torch.cat(actions_b)\n",
        "    t3 = torch.cat(rewards_b)\n",
        "    t4 = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
        "    t5 = torch.cat(dones_b)\n",
        "\n",
        "    return t1, t2, t3, t4, t5\n",
        "\n",
        "\n",
        "def get_loss(current_q_values, target_q_values, importance, batch_size, device):\n",
        "    \"\"\"\n",
        "    Returns the weighted loss by the given importance and the TD errors\n",
        "    \"\"\"\n",
        "    loss = (torch.tensor(importance, device=device) * F.smooth_l1_loss(current_q_values,\n",
        "                                                                       target_q_values.unsqueeze(1))).mean()\n",
        "    errors = abs(target_q_values.unsqueeze(1) - current_q_values.detach()).cpu().numpy().reshape(batch_size)\n",
        "    return errors, loss\n",
        "\n",
        "\n",
        "def get_euclidian_closest_bubble(game, bubbles_list):\n",
        "    \"\"\"\n",
        "    :return: euclidean distance of agent and closest bubble from bubbles_list\n",
        "    \"\"\"\n",
        "    bubbles_dist = [euclidean_dist_bubble_and_player(bubble, game.players[0]) for bubble in bubbles_list]\n",
        "    min_dist_bubble_index = int(np.argmin(np.array(bubbles_dist)))\n",
        "    return bubbles_list[min_dist_bubble_index], bubbles_dist[min_dist_bubble_index]\n",
        "\n",
        "\n",
        "def get_x_axis_closest_bubble(game, bubbles_list):\n",
        "    \"\"\"\n",
        "    :return: X axis distance of agent and closest bubble from bubbles_list\n",
        "    \"\"\"\n",
        "    bubbles_dist = [dist_from_bubble_and_player(bubble, game.players[0], axis=0) for bubble in bubbles_list]\n",
        "    min_dist_bubble_index = int(np.argmin(np.array(bubbles_dist)))\n",
        "    return bubbles_list[min_dist_bubble_index], bubbles_dist[min_dist_bubble_index]\n",
        "\n",
        "\n",
        "def dist_from_bubble_and_player(bubble, player, axis=0):\n",
        "    \"\"\"\n",
        "    :return: dist of player from bubble in axis\n",
        "    \"\"\"\n",
        "    if axis == 0:\n",
        "        player_spot = player.rect.centerx\n",
        "        pos_bubble_spot = bubble.rect.left\n",
        "        neg_bubble_spot = bubble.rect.right\n",
        "        return min(abs(player_spot - pos_bubble_spot), abs(player_spot - neg_bubble_spot))\n",
        "    else:\n",
        "        return abs(bubble.rect.bottom - player.rect.top)\n",
        "\n",
        "\n",
        "def euclidean_dist_bubble_and_player(bubble, player):\n",
        "    \"\"\"\n",
        "    :return: euclidean dist of player from bubble\n",
        "    \"\"\"\n",
        "    return math.sqrt(math.pow(dist_from_bubble_and_player(bubble, player, 0), 2) + math.pow(\n",
        "        dist_from_bubble_and_player(bubble, player, 1), 2))\n",
        "\n",
        "\n",
        "def plot(values, moving_avg_period, val_type):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel(val_type)\n",
        "    plt.plot(values, label=val_type)\n",
        "    moving_avg = get_moving_average(moving_avg_period, values)\n",
        "    plt.plot(moving_avg, label='Moving Average')\n",
        "    plt.legend()\n",
        "    plt.pause(0.001)\n",
        "    print(\"Episode\", len(values), \"\\n\", moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
        "\n",
        "\n",
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
        "            .mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykqzP2XZ62S",
        "colab_type": "text"
      },
      "source": [
        "# Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNmXZRJjZ3-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network Parameters\n",
        "NUM_FILTER_L1 = 32\n",
        "NUM_FILTER_L2 = 64\n",
        "NUM_FILTER_L3 = 64\n",
        "\n",
        "KER_L1 = 8\n",
        "KER_L2 = 4\n",
        "KER_L3 = 3\n",
        "\n",
        "STRIDE_L1 = 4\n",
        "STRIDE_L2 = 2\n",
        "STRIDE_L3 = 1\n",
        "\n",
        "# Shape of the network input (WIDTH,HEIGHT,NUM_OF_CHANNELS)\n",
        "WIDTH = 84\n",
        "HEIGHT = 84\n",
        "NUM_OF_CHANNELS = 3\n",
        "\n",
        "# Number of outputs\n",
        "NUM_OF_ACTIONS = 4\n",
        "\n",
        "# Training Parameters\n",
        "SKIP_FRAMES = 3\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.02\n",
        "EPS_DECAY = 70000\n",
        "LR = 0.00025\n",
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "TARGET_UPDATE = 10000\n",
        "MEMORY_SIZE = 20000 # If you have access to high RAM you may change to 90,000\n",
        "NUM_EPISODES = 10000\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2qf9YWFmyUF",
        "colab_type": "text"
      },
      "source": [
        "# Main : Run Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHhlJPhUYJ04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "em = EnvManager(device, NUM_OF_CHANNELS, SKIP_FRAMES, to_skip=True, ep_live=True)\n",
        "num_actions = em.num_actions_available()\n",
        "test_em = EnvManager(device, NUM_OF_CHANNELS, SKIP_FRAMES, to_skip=False, ep_live=False)\n",
        "policy_net, target_net = init_networks(NUM_OF_CHANNELS, num_actions, CNN, device)\n",
        "optimizer = optim.Adam(params=policy_net.parameters(), lr=LR)\n",
        "strategy = EpsilonGreedyStrategy(EPS_START, EPS_END, EPS_DECAY)\n",
        "agent = Agent(policy_net, target_net, strategy, em, test_em, em.num_actions_available(), optimizer)\n",
        "agent.train(NUM_EPISODES)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}